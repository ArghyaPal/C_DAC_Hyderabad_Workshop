{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two families of ensemble methods are usually distinguished:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In **averaging methods**, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n",
    "\n",
    "*Examples:* Bagging methods, Forests of randomized trees etc.\n",
    "\n",
    "* By contrast, in **boosting methods**, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n",
    "\n",
    "*Examples:* AdaBoost, Gradient Tree Boosting etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will study two types of boosting techniques: **Ada-Boosting and Gradient Boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting starts its journey with the question: **Can a set of weak learners create a single strong learner?** A weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ada-Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module *sklearn.ensemble* includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core principle of AdaBoost is to *fit a sequence of weak learners* (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through **a weighted majority vote (or sum) to produce the final prediction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data modifications at each so-called boosting iteration consist of applying weights w_1, w_2, ..., w_N to each of the training samples. Initially, those weights are all set to w_i = 1/N, so that the **first step** simply trains a weak learner on the original data. **For each successive iteration**, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. **At a given step**, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. **As iterations proceed**, examples that are difficult to predict receive ever-increasing influence. **Each subsequent** weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost can be used both for classification and regression problems:\n",
    "* For multi-class classification, AdaBoostClassifier implements AdaBoost-SAMME and AdaBoost-SAMME.R [ZZRH2009].\n",
    "* For regression, AdaBoostRegressor implements AdaBoost.R2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class sklearn.ensemble.AdaBoostClassifier(Parameters)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters | \n",
    "-----------|--------------------------------------------------------------------\n",
    "           | base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "           | n_estimators : integer, optional (default=50)\n",
    "           | learning_rate : float, optional (default=1.)\n",
    "           | algorithm : {‘SAMME’, ‘SAMME.R’}, optional (default=’SAMME.R’)\n",
    "           | random_state : int, RandomState instance or None, optional (default=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following *example* shows how to fit an AdaBoost classifier with 100 weak learners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.cross_validation import cross_val_score\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    ">>> iris = load_iris()\n",
    ">>> clf = AdaBoostClassifier(n_estimators=100)\n",
    ">>> scores = cross_val_score(clf, iris.data, iris.target)\n",
    ">>> scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **learning_rate** parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the base_estimator parameter. \n",
    "* The main parameters to tune to obtain good results are **n_estimators** and the complexity of the base estimators (e.g., its depth **max_depth** or minimum required number of samples at a leaf **min_samples_leaf** in case of decision trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "1. The number of weak learners is controlled by the parameter number of estimator i.e. **n_estimators**. Change the number of estimator and see how the end results differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Noel Dawe <noel.dawe@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from sklearn.externals.six.moves import zip\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "X, y = make_gaussian_quantiles(n_samples=13000, n_features=10,\n",
    "                               n_classes=3, random_state=1)\n",
    "\n",
    "n_split = 3000\n",
    "\n",
    "X_train, X_test = X[:n_split], X[n_split:]\n",
    "y_train, y_test = y[:n_split], y[n_split:]\n",
    "\n",
    "bdt_real = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=600,\n",
    "    learning_rate=1)\n",
    "\n",
    "bdt_discrete = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=600,\n",
    "    learning_rate=1.5,\n",
    "    algorithm=\"SAMME\")\n",
    "\n",
    "bdt_real.fit(X_train, y_train)\n",
    "bdt_discrete.fit(X_train, y_train)\n",
    "\n",
    "real_test_errors = []\n",
    "discrete_test_errors = []\n",
    "\n",
    "for real_test_predict, discrete_train_predict in zip(\n",
    "        bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)):\n",
    "    real_test_errors.append(\n",
    "        1. - accuracy_score(real_test_predict, y_test))\n",
    "    discrete_test_errors.append(\n",
    "        1. - accuracy_score(discrete_train_predict, y_test))\n",
    "\n",
    "n_trees_discrete = len(bdt_discrete)\n",
    "n_trees_real = len(bdt_real)\n",
    "\n",
    "# Boosting might terminate early, but the following arrays are always\n",
    "# n_estimators long. We crop them to the actual number of trees here:\n",
    "discrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete]\n",
    "real_estimator_errors = bdt_real.estimator_errors_[:n_trees_real]\n",
    "discrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(range(1, n_trees_discrete + 1),\n",
    "         discrete_test_errors, c='black', label='SAMME')\n",
    "plt.plot(range(1, n_trees_real + 1),\n",
    "         real_test_errors, c='black',\n",
    "         linestyle='dashed', label='SAMME.R')\n",
    "plt.legend()\n",
    "plt.ylim(0.18, 0.62)\n",
    "plt.ylabel('Test Error')\n",
    "plt.xlabel('Number of Trees')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_errors,\n",
    "         \"b\", label='SAMME', alpha=.5)\n",
    "plt.plot(range(1, n_trees_real + 1), real_estimator_errors,\n",
    "         \"r\", label='SAMME.R', alpha=.5)\n",
    "plt.legend()\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylim((.2,\n",
    "         max(real_estimator_errors.max(),\n",
    "             discrete_estimator_errors.max()) * 1.2))\n",
    "plt.xlim((-20, len(bdt_discrete) + 20))\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights,\n",
    "         \"b\", label='SAMME')\n",
    "plt.legend()\n",
    "plt.ylabel('Weight')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylim((0, discrete_estimator_weights.max() * 1.2))\n",
    "plt.xlim((-20, n_trees_discrete + 20))\n",
    "\n",
    "# prevent overlapping y-axis labels\n",
    "plt.subplots_adjust(wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "1. Change n_samples, n_features, n_classes, random_state and see the result? Are those affecting the total running time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Tree Boosting** or **Gradient Boosted Regression Trees** (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient Tree Boosting models are used in a variety of areas including Web search ranking and ecology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages of GBRT are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Natural handling of data of mixed type (= heterogeneous features)\n",
    "* Predictive power\n",
    "* Robustness to outliers in output space (via robust loss functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantages of GBRT are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scalability, due to the sequential nature of boosting it can hardly be parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module *sklearn.ensemble* provides methods for both classification and regression via gradient boosted regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. **Classification:** GradientBoostingClassifier supports both binary and multi-class classification. The following example shows how to fit a gradient boosting classifier with 100 decision stumps as weak learners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.datasets import make_hastie_10_2\n",
    ">>> from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    ">>> X, y = make_hastie_10_2(random_state=0)\n",
    ">>> X_train, X_test = X[:2000], X[2000:]\n",
    ">>> y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    ">>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
    ">>> clf.score(X_test, y_test)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "* The number of weak learners (i.e. regression trees) is controlled by the parameter n_estimators. Change that and see the result.\n",
    "* Change the parameter max_depth, you can see that the size of each tree can be controlled by this parameter.\n",
    "* The learning_rate is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via shrinkage, change that parameter and see the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. **Regression:** GradientBoostingRegressor supports a number of different loss functions for regression which can be specified via the argument loss; the default loss function for regression is least squares ('ls')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following loss functions are supported and can be specified using the parameter loss:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regression\n",
    "            1. Least squares ('ls'): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.\n",
    "            2. Least absolute deviation ('lad'): A robust loss function for regression. The initial model is given by the median of the target values.\n",
    "            3. Huber ('huber'): Another robust loss function that combines least squares and least absolute deviation; use alpha to control the sensitivity with regards to outliers (see [F2001] for more details).\n",
    "            4. Quantile ('quantile'): A loss function for quantile regression. Use 0 < alpha < 1 to specify the quantile. This loss function can be used to create prediction intervals (see Prediction Intervals for Gradient Boosting Regression).\n",
    "* Classification\n",
    "            1. Binomial deviance ('deviance'): The negative binomial log-likelihood loss function for binary classification (provides probability estimates). The initial model is given by the log odds-ratio.\n",
    "            2. Multinomial deviance ('deviance'): The negative multinomial log-likelihood loss function for multi-class classification with n_classes mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration n_classes regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.\n",
    "            3. Exponential loss ('exponential'): The same loss function as AdaBoostClassifier. Less robust to mislabeled examples than 'deviance'; can only be used for binary classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.metrics import mean_squared_error\n",
    ">>> from sklearn.datasets import make_friedman1\n",
    ">>> from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    ">>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    ">>> X_train, X_test = X[:200], X[200:]\n",
    ">>> y_train, y_test = y[:200], y[200:]\n",
    ">>> est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "...     max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n",
    ">>> mean_squared_error(y_test, est.predict(X_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "* Change the loss function and see the performance change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both *GradientBoostingRegressor* and *GradientBoostingClassifier* support **warm_start=True** which allows you to add more estimators to an already fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. **Controlling the tree size:** The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth h can capture interactions of order h . There are two ways in which the size of the individual regression trees can be controlled.\n",
    "\n",
    "If you specify max_depth=h then complete binary trees of depth h will be grown. Such trees will have (at most) 2^h leaf nodes and 2^h - 1 split nodes.\n",
    "\n",
    "Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter max_leaf_nodes. In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with max_leaf_nodes=k has k - 1 split nodes and thus can model interactions of up to order max_leaf_nodes - 1 .\n",
    "\n",
    "We found that max_leaf_nodes=k gives comparable results to max_depth=k-1 but is significantly faster to train at the expense of a slightly higher training error. The parameter max_leaf_nodes corresponds to the variable J in the chapter on gradient boosting in [F2001] and is related to the parameter interaction.depth in R’s gbm package where max_leaf_nodes == interaction.depth + 1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. **Regularization:** a simple regularization strategy that scales the contribution of each weak learner by a factor \\nu:\n",
    "\n",
    "F_m(x) = F_{m-1}(x) + \\nu \\gamma_m h_m(x)\n",
    "\n",
    "The parameter \\nu is also called the learning rate because it scales the step length the the gradient descent procedure; it can be set via the learning_rate parameter.\n",
    "\n",
    "The parameter learning_rate strongly interacts with the parameter n_estimators, the number of weak learners to fit. Smaller values of learning_rate require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of learning_rate favor better test error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. **Interpretation:** Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models, however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature importance: Often features do not contribute equally to predict the target response; in many situations the majority of the features are in fact irrelevant. When interpreting a model, the first question usually is: what are those important features and how do they contributing in predicting the target response?\n",
    "\n",
    "Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the feature importance of each tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance scores of a fit gradient boosting model can be accessed via the feature_importances_ property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.datasets import make_hastie_10_2\n",
    ">>> from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    ">>> X, y = make_hastie_10_2(random_state=0)\n",
    ">>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "...     max_depth=1, random_state=0).fit(X, y)\n",
    ">>> clf.feature_importances_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
